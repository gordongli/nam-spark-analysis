{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up our Schema\n",
    "\n",
    "Spark can automatically create a schema for CSV files, but ours don't have headings. Let's set this up here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(Timestamp,LongType,true),StructField(Geohash,StringType,true),StructField(geopotential_height_lltw,FloatType,true),StructField(water_equiv_of_accum_snow_depth_surface,FloatType,true),StructField(drag_coefficient_surface,FloatType,true),StructField(sensible_heat_net_flux_surface,FloatType,true),StructField(categorical_ice_pellets_yes1_no0_surface,FloatType,true),StructField(visibility_surface,FloatType,true),StructField(number_of_soil_layers_in_root_zone_surface,FloatType,true),StructField(categorical_freezing_rain_yes1_no0_surface,FloatType,true),StructField(pressure_reduced_to_msl_msl,FloatType,true),StructField(upward_short_wave_rad_flux_surface,FloatType,true),StructField(relative_humidity_zerodegc_isotherm,FloatType,true),StructField(categorical_snow_yes1_no0_surface,FloatType,true),StructField(u-component_of_wind_tropopause,FloatType,true),StructField(surface_wind_gust_surface,FloatType,true),StructField(total_cloud_cover_entire_atmosphere,FloatType,true),StructField(upward_long_wave_rad_flux_surface,FloatType,true),StructField(land_cover_land1_sea0_surface,FloatType,true),StructField(vegitation_type_as_in_sib_surface,FloatType,true),StructField(v-component_of_wind_pblri,FloatType,true),StructField(albedo_surface,FloatType,true),StructField(lightning_surface,FloatType,true),StructField(ice_cover_ice1_no_ice0_surface,FloatType,true),StructField(convective_inhibition_surface,FloatType,true),StructField(pressure_surface,FloatType,true),StructField(transpiration_stress-onset_soil_moisture_surface,FloatType,true),StructField(soil_porosity_surface,FloatType,true),StructField(vegetation_surface,FloatType,true),StructField(categorical_rain_yes1_no0_surface,FloatType,true),StructField(downward_long_wave_rad_flux_surface,FloatType,true),StructField(planetary_boundary_layer_height_surface,FloatType,true),StructField(soil_type_as_in_zobler_surface,FloatType,true),StructField(geopotential_height_cloud_base,FloatType,true),StructField(friction_velocity_surface,FloatType,true),StructField(maximumcomposite_radar_reflectivity_entire_atmosphere,FloatType,true),StructField(plant_canopy_surface_water_surface,FloatType,true),StructField(v-component_of_wind_maximum_wind,FloatType,true),StructField(geopotential_height_zerodegc_isotherm,FloatType,true),StructField(mean_sea_level_pressure_nam_model_reduction_msl,FloatType,true),StructField(temperature_surface,FloatType,true),StructField(snow_cover_surface,FloatType,true),StructField(geopotential_height_surface,FloatType,true),StructField(convective_available_potential_energy_surface,FloatType,true),StructField(latent_heat_net_flux_surface,FloatType,true),StructField(surface_roughness_surface,FloatType,true),StructField(pressure_maximum_wind,FloatType,true),StructField(temperature_tropopause,FloatType,true),StructField(geopotential_height_pblri,FloatType,true),StructField(pressure_tropopause,FloatType,true),StructField(snow_depth_surface,FloatType,true),StructField(v-component_of_wind_tropopause,FloatType,true),StructField(downward_short_wave_rad_flux_surface,FloatType,true),StructField(u-component_of_wind_maximum_wind,FloatType,true),StructField(wilting_point_surface,FloatType,true),StructField(precipitable_water_entire_atmosphere,FloatType,true),StructField(u-component_of_wind_pblri,FloatType,true),StructField(direct_evaporation_cease_soil_moisture_surface,FloatType,true)))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, FloatType, LongType, StringType\n",
    "\n",
    "feats = []\n",
    "f = open('features.txt')\n",
    "for line_num, line in enumerate(f):\n",
    "    if line_num == 0:\n",
    "        # Timestamp\n",
    "        feats.append(StructField(line.strip(), LongType(), True))\n",
    "    elif line_num == 1:\n",
    "        # Geohash\n",
    "        feats.append(StructField(line.strip(), StringType(), True))\n",
    "    else:\n",
    "        # Other features\n",
    "        feats.append(StructField(line.strip(), FloatType(), True))\n",
    "    \n",
    "schema = StructType(feats)\n",
    "\n",
    "print(schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Dataframe\n",
    "\n",
    "Let's load our CSV into a 'dataframe' - Spark's abstraction for working with tabular data (built on top of RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nam_t = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:15000/nam_tiny.tdv')\n",
    "nam_s = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:15000/nam_s/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Timestamp: bigint, Geohash: string, geopotential_height_lltw: float, water_equiv_of_accum_snow_depth_surface: float, drag_coefficient_surface: float, sensible_heat_net_flux_surface: float, categorical_ice_pellets_yes1_no0_surface: float, visibility_surface: float, number_of_soil_layers_in_root_zone_surface: float, categorical_freezing_rain_yes1_no0_surface: float, pressure_reduced_to_msl_msl: float, upward_short_wave_rad_flux_surface: float, relative_humidity_zerodegc_isotherm: float, categorical_snow_yes1_no0_surface: float, u-component_of_wind_tropopause: float, surface_wind_gust_surface: float, total_cloud_cover_entire_atmosphere: float, upward_long_wave_rad_flux_surface: float, land_cover_land1_sea0_surface: float, vegitation_type_as_in_sib_surface: float, v-component_of_wind_pblri: float, albedo_surface: float, lightning_surface: float, ice_cover_ice1_no_ice0_surface: float, convective_inhibition_surface: float, pressure_surface: float, transpiration_stress-onset_soil_moisture_surface: float, soil_porosity_surface: float, vegetation_surface: float, categorical_rain_yes1_no0_surface: float, downward_long_wave_rad_flux_surface: float, planetary_boundary_layer_height_surface: float, soil_type_as_in_zobler_surface: float, geopotential_height_cloud_base: float, friction_velocity_surface: float, maximumcomposite_radar_reflectivity_entire_atmosphere: float, plant_canopy_surface_water_surface: float, v-component_of_wind_maximum_wind: float, geopotential_height_zerodegc_isotherm: float, mean_sea_level_pressure_nam_model_reduction_msl: float, temperature_surface: float, snow_cover_surface: float, geopotential_height_surface: float, convective_available_potential_energy_surface: float, latent_heat_net_flux_surface: float, surface_roughness_surface: float, pressure_maximum_wind: float, temperature_tropopause: float, geopotential_height_pblri: float, pressure_tropopause: float, snow_depth_surface: float, v-component_of_wind_tropopause: float, downward_short_wave_rad_flux_surface: float, u-component_of_wind_maximum_wind: float, wilting_point_surface: float, precipitable_water_entire_atmosphere: float, u-component_of_wind_pblri: float, direct_evaporation_cease_soil_moisture_surface: float]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nam_t.cache()\n",
    "nam_s.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an SQL 'table'\n",
    "nam_t.createOrReplaceTempView(\"nam_t\")\n",
    "nam_s.createOrReplaceTempView(\"nam_s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.cacheTable(\"nam_t\")\n",
    "spark.catalog.cacheTable(\"nam_s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unknown Feature\n",
    "I didn't know what albedo was, so I looked at its summary statistics. Still unsure, I looked up the definition: 'the proportion of the incident light or radiation that is reflected by a surface, typically that of a planet or moon.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|summary|  albedo_surface|\n",
      "+-------+----------------+\n",
      "|  count|             100|\n",
      "|   mean|           18.07|\n",
      "| stddev|17.4802948221907|\n",
      "|    min|             6.0|\n",
      "|    max|            76.0|\n",
      "+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nam_t.describe('albedo_surface').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hot Hot Hot\n",
    "The hottest tempurature in the dataset is 330.67431640625 at location d5f0jqerq27bat (21.13070154, -86.9520505; Benito Ju√°rez, Quintana Roo, Mexico) at time 2015-08-23T18:00Z. This record is not surprising, as it is near the equator in the summer. Looking at the other highest tempuratures which are near this highest one, it does not appear to be an anomaly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Timestamp: bigint, Geohash: string, geopotential_height_lltw: float, water_equiv_of_accum_snow_depth_surface: float, drag_coefficient_surface: float, sensible_heat_net_flux_surface: float, categorical_ice_pellets_yes1_no0_surface: float, visibility_surface: float, number_of_soil_layers_in_root_zone_surface: float, categorical_freezing_rain_yes1_no0_surface: float, pressure_reduced_to_msl_msl: float, upward_short_wave_rad_flux_surface: float, relative_humidity_zerodegc_isotherm: float, categorical_snow_yes1_no0_surface: float, u-component_of_wind_tropopause: float, surface_wind_gust_surface: float, total_cloud_cover_entire_atmosphere: float, upward_long_wave_rad_flux_surface: float, land_cover_land1_sea0_surface: float, vegitation_type_as_in_sib_surface: float, v-component_of_wind_pblri: float, albedo_surface: float, lightning_surface: float, ice_cover_ice1_no_ice0_surface: float, convective_inhibition_surface: float, pressure_surface: float, transpiration_stress-onset_soil_moisture_surface: float, soil_porosity_surface: float, vegetation_surface: float, categorical_rain_yes1_no0_surface: float, downward_long_wave_rad_flux_surface: float, planetary_boundary_layer_height_surface: float, soil_type_as_in_zobler_surface: float, geopotential_height_cloud_base: float, friction_velocity_surface: float, maximumcomposite_radar_reflectivity_entire_atmosphere: float, plant_canopy_surface_water_surface: float, v-component_of_wind_maximum_wind: float, geopotential_height_zerodegc_isotherm: float, mean_sea_level_pressure_nam_model_reduction_msl: float, temperature_surface: float, snow_cover_surface: float, geopotential_height_surface: float, convective_available_potential_energy_surface: float, latent_heat_net_flux_surface: float, surface_roughness_surface: float, pressure_maximum_wind: float, temperature_tropopause: float, geopotential_height_pblri: float, pressure_tropopause: float, snow_depth_surface: float, v-component_of_wind_tropopause: float, downward_short_wave_rad_flux_surface: float, u-component_of_wind_maximum_wind: float, wilting_point_surface: float, precipitable_water_entire_atmosphere: float, u-component_of_wind_pblri: float, direct_evaporation_cease_soil_moisture_surface: float]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_desc = nam_s.orderBy(\"temperature_surface\", ascending=False)\n",
    "temp_desc.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hottest_record = temp_desc.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330.67431640625"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hottest_record['temperature_surface']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2015, 8, 23, 18, 0, tzinfo=datetime.timezone.utc)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "timestamp = hottest_record['Timestamp'] / 1000\n",
    "datetime.fromtimestamp(timestamp, timezone.utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d5f0jqerq27b'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hottest_record['Geohash']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(temperature_surface=330.67431640625, Timestamp=1440352800000, Geohash='d5f0jqerq27b'),\n",
       " Row(temperature_surface=330.640625, Timestamp=1440266400000, Geohash='d5f0vd8eb80p'),\n",
       " Row(temperature_surface=330.6044921875, Timestamp=1430157600000, Geohash='9g77js659k20'),\n",
       " Row(temperature_surface=330.53662109375, Timestamp=1439056800000, Geohash='d5f0jqerq27b'),\n",
       " Row(temperature_surface=330.48193359375, Timestamp=1440612000000, Geohash='d59d5yttuc5b'),\n",
       " Row(temperature_surface=330.35693359375, Timestamp=1440612000000, Geohash='d59eqv7e03pb'),\n",
       " Row(temperature_surface=330.23193359375, Timestamp=1440612000000, Geohash='d59dntd726gz'),\n",
       " Row(temperature_surface=330.220703125, Timestamp=1440698400000, Geohash='d59eqv7e03pb'),\n",
       " Row(temperature_surface=330.179931640625, Timestamp=1438279200000, Geohash='d5f04xyhucez'),\n",
       " Row(temperature_surface=330.14990234375, Timestamp=1439488800000, Geohash='d5dpds10m55b')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_desc.select(\"temperature_surface\", \"Timestamp\", \"Geohash\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So Snowy\n",
    "One location where it is snowing all year, i.e. where it is snowing for every record in its timeseries is {location}.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(geohash='c1nuq5290jup', sum=1.0, cnt=1, div=1.0),\n",
       " Row(geohash='f2w29r4werxb', sum=1.0, cnt=1, div=1.0),\n",
       " Row(geohash='f2d5v1jeyp7z', sum=1.0, cnt=1, div=1.0),\n",
       " Row(geohash='c6s64488ws80', sum=1.0, cnt=1, div=1.0),\n",
       " Row(geohash='f2fh6jpdgv5b', sum=1.0, cnt=1, div=1.0),\n",
       " Row(geohash='fccz22w4fytb', sum=1.0, cnt=1, div=1.0),\n",
       " Row(geohash='drmg1tprm22p', sum=0.0, cnt=1, div=0.0),\n",
       " Row(geohash='8gzfnvh85g0p', sum=0.0, cnt=1, div=0.0),\n",
       " Row(geohash='8up5c0e570pz', sum=0.0, cnt=1, div=0.0),\n",
       " Row(geohash='dse39p1jkszz', sum=0.0, cnt=1, div=0.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gdf = df.groupBy('Geohash')\n",
    "# gdf.agg(sum('categorical_snow_yes1_no0_surface'), count('*')).orderBy('sum(categorical_snow_yes1_no0_surface)', ascending = False).collect()\n",
    "\n",
    "so_snowy = spark.sql(\"SELECT geohash, sum, cnt, (sum / cnt) AS div \\\n",
    "            FROM ( \\\n",
    "                SELECT geohash, SUM(categorical_snow_yes1_no0_surface) AS sum, COUNT(*) as cnt \\\n",
    "                FROM nam_tiny \\\n",
    "                GROUP BY(geohash) \\\n",
    "            ) ORDER BY div DESC\")\n",
    "so_snowy.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strangely Snowy \n",
    "Find a location that contains snow while its surroundings do not. Why does this occur? Is it a high mountain peak in a desert?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning rod\n",
    "Where are you most likely to be struck by lightning? Use a precision of at least 4 Geohash characters and provide the top 3 locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_rod = spark.sql(\"\\\n",
    "                           SELECT LEFT(geohash, 4) AS geo4, SUM(lightning_surface) / COUNT(*) AS prob_lightning \\\n",
    "                           FROM nam_tiny \\\n",
    "                           GROUP BY geo4 \\\n",
    "                           ORDER BY prob_lightning DESC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(geo4='dmbz', prob_lightning=1.0),\n",
       " Row(geo4='c4xg', prob_lightning=0.0),\n",
       " Row(geo4='d5x3', prob_lightning=0.0)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightning_rod.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drying out\n",
    "Choose a region in North America (defined by one or more Geohashes) and determine when its driest month is. This should include a histogram with data from each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from_unixtime(timestamp DIV 1000) as timestring, \\\n",
    "\n",
    "drying_out = spark.sql(\"\\\n",
    "    SELECT \\\n",
    "        trunc(from_unixtime(timestamp DIV 1000), 'MM') as truncated \\\n",
    "    FROM nam_tiny \\\n",
    "    GROUP BY truncated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| truncated|\n",
      "+----------+\n",
      "|2015-03-01|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drying_out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travel Startup\n",
    "After graduating from USF, you found a startup that aims to provide personalized travel itineraries using big data analysis. Given your own personal preferences, build a plan for a year of travel across 5 locations. Or, in other words: pick 5 regions. What is the best time of year to visit them based on the dataset?\n",
    "\n",
    "- One avenue here could be determining the comfort index for a region. You could incorporate several features: not too hot, not too cold, dry, humid, windy, etc. There are several different ways of calculating this available online, and you could also analyze how well your own metrics do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escaping the fog\n",
    "After becoming rich from your startup, you are looking for the perfect location to build your Bay Area mansion with unobstructed views. Find the locations that are the least foggy and show them on a map.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\\\n",
    "    SELECT LEFT(geohash) as geo4, SUM(visibility_surface) /  \\\n",
    "    FROM nam_tiny \\\n",
    "    GROUP BY truncated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SolarWind, Inc.\n",
    "You get bored enjoying the amazing views from your mansion, so you start a new company; here, you want to help power companies plan out the locations of solar and wind farms across North America. Locate the top 3 places for solar and wind farms, as well as a combination of both (solar + wind farm). You will report a total of 9 Geohashes as well as their relevant attributes (for example, cloud cover and wind speeds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even create a sample dataset with Spark! Let's create a 10% sample (without replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = df.sample(False, .1)\n",
    "\n",
    "# Write it out to a file\n",
    "samp.write.format('csv').save('hdfs://orion12:50000/sampled_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayarea_tiny = spark.sql(\"\\\n",
    "                           SELECT * \\\n",
    "                           FROM nam_tiny \\\n",
    "                           WHERE LEFT(geohash, 3) IN ('9qb', '9qc', '9q9', '9q8')\")\n",
    "\n",
    "\n",
    "nine_tiny = spark.sql(\"\\\n",
    "                           SELECT * \\\n",
    "                           FROM nam_tiny \\\n",
    "                           WHERE LEFT(geohash, 1) IN ('9', '8')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayarea_small = spark.sql(\"\\\n",
    "                           SELECT * \\\n",
    "                           FROM nam_small \\\n",
    "                           WHERE LEFT(geohash, 3) IN ('9qb', '9qc', '9q9', '9q8')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayarea_full = spark.sql(\"\\\n",
    "SELECT * \\\n",
    "FROM nam_full \\\n",
    "WHERE LEFT(geohash, 4) IN ( \\\n",
    " '9q8x', '9q8z', '9q8y', '9q8v', '9q8u', '9q8g', \\\n",
    " '9q9p', '9q9n', '9q9j', '9q9h', '9q95', '9q9r', '9q9q', '9q9m', '9q9k', '9q97', '9q9e', \\\n",
    " '9qb3', '9q95', '9qb6', '9qb7', '9qb8', '9qb9', '9qbb', '9qbc', '9qbd', '9qbe', '9qbf', '9qbg', '9qbh', '9qbk', '9qbs', \\\n",
    " '9qc0', '9qc1', '9qc2', '9qc3', '9qc4', '9qc5', '9qc6', '9qch' \\\n",
    ")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayarea_full.write.format('csv').save('hdfs://orion11:15000/nam_bayarea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayarea_full.createOrReplaceTempView(\"nam_bayarea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
