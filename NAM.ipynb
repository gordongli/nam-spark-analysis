{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up our Schema\n",
    "\n",
    "Spark can automatically create a schema for CSV files, but ours don't have headings. Let's set this up here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(Timestamp,LongType,true),StructField(Geohash,StringType,true),StructField(geopotential_height_lltw,FloatType,true),StructField(water_equiv_of_accum_snow_depth_surface,FloatType,true),StructField(drag_coefficient_surface,FloatType,true),StructField(sensible_heat_net_flux_surface,FloatType,true),StructField(categorical_ice_pellets_yes1_no0_surface,FloatType,true),StructField(visibility_surface,FloatType,true),StructField(number_of_soil_layers_in_root_zone_surface,FloatType,true),StructField(categorical_freezing_rain_yes1_no0_surface,FloatType,true),StructField(pressure_reduced_to_msl_msl,FloatType,true),StructField(upward_short_wave_rad_flux_surface,FloatType,true),StructField(relative_humidity_zerodegc_isotherm,FloatType,true),StructField(categorical_snow_yes1_no0_surface,FloatType,true),StructField(u-component_of_wind_tropopause,FloatType,true),StructField(surface_wind_gust_surface,FloatType,true),StructField(total_cloud_cover_entire_atmosphere,FloatType,true),StructField(upward_long_wave_rad_flux_surface,FloatType,true),StructField(land_cover_land1_sea0_surface,FloatType,true),StructField(vegitation_type_as_in_sib_surface,FloatType,true),StructField(v-component_of_wind_pblri,FloatType,true),StructField(albedo_surface,FloatType,true),StructField(lightning_surface,FloatType,true),StructField(ice_cover_ice1_no_ice0_surface,FloatType,true),StructField(convective_inhibition_surface,FloatType,true),StructField(pressure_surface,FloatType,true),StructField(transpiration_stress-onset_soil_moisture_surface,FloatType,true),StructField(soil_porosity_surface,FloatType,true),StructField(vegetation_surface,FloatType,true),StructField(categorical_rain_yes1_no0_surface,FloatType,true),StructField(downward_long_wave_rad_flux_surface,FloatType,true),StructField(planetary_boundary_layer_height_surface,FloatType,true),StructField(soil_type_as_in_zobler_surface,FloatType,true),StructField(geopotential_height_cloud_base,FloatType,true),StructField(friction_velocity_surface,FloatType,true),StructField(maximumcomposite_radar_reflectivity_entire_atmosphere,FloatType,true),StructField(plant_canopy_surface_water_surface,FloatType,true),StructField(v-component_of_wind_maximum_wind,FloatType,true),StructField(geopotential_height_zerodegc_isotherm,FloatType,true),StructField(mean_sea_level_pressure_nam_model_reduction_msl,FloatType,true),StructField(temperature_surface,FloatType,true),StructField(snow_cover_surface,FloatType,true),StructField(geopotential_height_surface,FloatType,true),StructField(convective_available_potential_energy_surface,FloatType,true),StructField(latent_heat_net_flux_surface,FloatType,true),StructField(surface_roughness_surface,FloatType,true),StructField(pressure_maximum_wind,FloatType,true),StructField(temperature_tropopause,FloatType,true),StructField(geopotential_height_pblri,FloatType,true),StructField(pressure_tropopause,FloatType,true),StructField(snow_depth_surface,FloatType,true),StructField(v-component_of_wind_tropopause,FloatType,true),StructField(downward_short_wave_rad_flux_surface,FloatType,true),StructField(u-component_of_wind_maximum_wind,FloatType,true),StructField(wilting_point_surface,FloatType,true),StructField(precipitable_water_entire_atmosphere,FloatType,true),StructField(u-component_of_wind_pblri,FloatType,true),StructField(direct_evaporation_cease_soil_moisture_surface,FloatType,true)))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, FloatType, LongType, StringType\n",
    "\n",
    "feats = []\n",
    "f = open('features.txt')\n",
    "for line_num, line in enumerate(f):\n",
    "    if line_num == 0:\n",
    "        # Timestamp\n",
    "        feats.append(StructField(line.strip(), LongType(), True))\n",
    "    elif line_num == 1:\n",
    "        # Geohash\n",
    "        feats.append(StructField(line.strip(), StringType(), True))\n",
    "    else:\n",
    "        # Other features\n",
    "        feats.append(StructField(line.strip(), FloatType(), True))\n",
    "    \n",
    "schema = StructType(feats)\n",
    "\n",
    "print(schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Dataframe\n",
    "\n",
    "Let's load our CSV into a 'dataframe' - Spark's abstraction for working with tabular data (built on top of RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Timestamp=1426377600000, Geohash='953rtrfmww20', geopotential_height_lltw=3975.03125, water_equiv_of_accum_snow_depth_surface=0.0, drag_coefficient_surface=0.0, sensible_heat_net_flux_surface=10.42236328125, categorical_ice_pellets_yes1_no0_surface=0.0, visibility_surface=24221.587890625, number_of_soil_layers_in_root_zone_surface=0.0, categorical_freezing_rain_yes1_no0_surface=0.0, pressure_reduced_to_msl_msl=101366.0, upward_short_wave_rad_flux_surface=18.375, relative_humidity_zerodegc_isotherm=30.0, categorical_snow_yes1_no0_surface=0.0, u-component_of_wind_tropopause=5.4028778076171875, surface_wind_gust_surface=3.15878963470459, total_cloud_cover_entire_atmosphere=100.0, upward_long_wave_rad_flux_surface=424.9310302734375, land_cover_land1_sea0_surface=0.0, vegitation_type_as_in_sib_surface=0.0, v-component_of_wind_pblri=-1.61834716796875, albedo_surface=6.0, lightning_surface=0.0, ice_cover_ice1_no_ice0_surface=0.0, convective_inhibition_surface=-0.65234375, pressure_surface=101366.0, transpiration_stress-onset_soil_moisture_surface=0.0, soil_porosity_surface=0.0, vegetation_surface=0.0, categorical_rain_yes1_no0_surface=0.0, downward_long_wave_rad_flux_surface=414.9662780761719, planetary_boundary_layer_height_surface=1800.5, soil_type_as_in_zobler_surface=0.0, geopotential_height_cloud_base=500.0, friction_velocity_surface=0.12586598098278046, maximumcomposite_radar_reflectivity_entire_atmosphere=7.625, plant_canopy_surface_water_surface=0.0, v-component_of_wind_maximum_wind=-2.013824462890625, geopotential_height_zerodegc_isotherm=5080.0, mean_sea_level_pressure_nam_model_reduction_msl=101367.0, temperature_surface=296.4980163574219, snow_cover_surface=0.0, geopotential_height_surface=0.0700225830078125, convective_available_potential_energy_surface=40.0, latent_heat_net_flux_surface=67.96708679199219, surface_roughness_surface=1.5900002836133353e-05, pressure_maximum_wind=16726.736328125, temperature_tropopause=195.10687255859375, geopotential_height_pblri=1106.427734375, pressure_tropopause=9182.23828125, snow_depth_surface=0.0, v-component_of_wind_tropopause=1.4414520263671875, downward_short_wave_rad_flux_surface=253.75, u-component_of_wind_maximum_wind=17.16766357421875, wilting_point_surface=0.0, precipitable_water_entire_atmosphere=24.707815170288086, u-component_of_wind_pblri=-3.038421630859375, direct_evaporation_cease_soil_moisture_surface=0.0)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('/Volumes/evo/Datasets/NAM_2015_S/*')\n",
    "df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:15000/nam_tiny.tdv')\n",
    "# df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:15000/nam_s/*')\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-17b90b4246d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreally_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature_surface\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m320\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreally_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhot_and_humid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature_surface\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m313\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelative_humidity_zerodegc_isotherm\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhot_and_humid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \"\"\"\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.3.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark-2.3.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.3.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home2/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "really_hot = df.filter(df.temperature_surface > 320).count()\n",
    "print(really_hot)\n",
    "\n",
    "hot_and_humid = df.filter(df.temperature_surface > 313).filter(df.relative_humidity_zerodegc_isotherm > .8).count()\n",
    "print(hot_and_humid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Timestamp=1426377600000, Geohash='cf7ecr4h2ps0', geopotential_height_lltw=136.53125, water_equiv_of_accum_snow_depth_surface=77.0, drag_coefficient_surface=0.0, sensible_heat_net_flux_surface=-39.57763671875, categorical_ice_pellets_yes1_no0_surface=0.0, visibility_surface=24221.587890625, number_of_soil_layers_in_root_zone_surface=3.0, categorical_freezing_rain_yes1_no0_surface=0.0, pressure_reduced_to_msl_msl=99602.0, upward_short_wave_rad_flux_surface=6.625, relative_humidity_zerodegc_isotherm=34.0, categorical_snow_yes1_no0_surface=0.0, u-component_of_wind_tropopause=27.527877807617188, surface_wind_gust_surface=16.158788681030273, total_cloud_cover_entire_atmosphere=100.0, upward_long_wave_rad_flux_surface=314.0560302734375, land_cover_land1_sea0_surface=1.0, vegitation_type_as_in_sib_surface=18.0, v-component_of_wind_pblri=12.31915283203125, albedo_surface=38.75, lightning_surface=0.0, ice_cover_ice1_no_ice0_surface=0.0, convective_inhibition_surface=-0.65234375, pressure_surface=97221.0, transpiration_stress-onset_soil_moisture_surface=0.3125, soil_porosity_surface=0.5, vegetation_surface=1.0, categorical_rain_yes1_no0_surface=0.0, downward_long_wave_rad_flux_surface=274.9662780761719, planetary_boundary_layer_height_surface=1905.5, soil_type_as_in_zobler_surface=3.0, geopotential_height_cloud_base=6696.0, friction_velocity_surface=0.6008660197257996, maximumcomposite_radar_reflectivity_entire_atmosphere=-20.0, plant_canopy_surface_water_surface=0.10999999940395355, v-component_of_wind_maximum_wind=-7.388824462890625, geopotential_height_zerodegc_isotherm=2380.0, mean_sea_level_pressure_nam_model_reduction_msl=99607.0, temperature_surface=272.7480163574219, snow_cover_surface=100.0, geopotential_height_surface=195.8200225830078, convective_available_potential_energy_surface=0.0, latent_heat_net_flux_surface=-0.0329132080078125, surface_roughness_surface=0.1750158965587616, pressure_maximum_wind=20726.736328125, temperature_tropopause=206.35687255859375, geopotential_height_pblri=300.42779541015625, pressure_tropopause=20582.23828125, snow_depth_surface=0.4211999773979187, v-component_of_wind_tropopause=-7.8085479736328125, downward_short_wave_rad_flux_surface=17.0, u-component_of_wind_maximum_wind=28.66766357421875, wilting_point_surface=0.04749999940395355, precipitable_water_entire_atmosphere=9.207815170288086, u-component_of_wind_pblri=4.711578369140625, direct_evaporation_cease_soil_moisture_surface=0.04749999940395355),\n",
       " Row(Timestamp=1426377600000, Geohash='ccn3u17cxe80', geopotential_height_lltw=4.28125, water_equiv_of_accum_snow_depth_surface=26.0, drag_coefficient_surface=0.0, sensible_heat_net_flux_surface=-12.95263671875, categorical_ice_pellets_yes1_no0_surface=0.0, visibility_surface=24221.587890625, number_of_soil_layers_in_root_zone_surface=4.0, categorical_freezing_rain_yes1_no0_surface=0.0, pressure_reduced_to_msl_msl=101712.0, upward_short_wave_rad_flux_surface=1.75, relative_humidity_zerodegc_isotherm=10.0, categorical_snow_yes1_no0_surface=0.0, u-component_of_wind_tropopause=16.527877807617188, surface_wind_gust_surface=6.90878963470459, total_cloud_cover_entire_atmosphere=0.0, upward_long_wave_rad_flux_surface=303.1810302734375, land_cover_land1_sea0_surface=1.0, vegitation_type_as_in_sib_surface=1.0, v-component_of_wind_pblri=5.63165283203125, albedo_surface=24.5, lightning_surface=0.0, ice_cover_ice1_no_ice0_surface=0.0, convective_inhibition_surface=-0.65234375, pressure_surface=96528.0, transpiration_stress-onset_soil_moisture_surface=0.3125, soil_porosity_surface=0.5, vegetation_surface=15.75, categorical_rain_yes1_no0_surface=0.0, downward_long_wave_rad_flux_surface=231.71627807617188, planetary_boundary_layer_height_surface=407.0, soil_type_as_in_zobler_surface=3.0, geopotential_height_cloud_base=-5000.0, friction_velocity_surface=0.3508659601211548, maximumcomposite_radar_reflectivity_entire_atmosphere=-20.0, plant_canopy_surface_water_surface=0.5, v-component_of_wind_maximum_wind=-34.263824462890625, geopotential_height_zerodegc_isotherm=2440.0, mean_sea_level_pressure_nam_model_reduction_msl=101723.0, temperature_surface=270.3730163574219, snow_cover_surface=100.0, geopotential_height_surface=426.32000732421875, convective_available_potential_energy_surface=0.0, latent_heat_net_flux_surface=-0.0329132080078125, surface_roughness_surface=1.9000158309936523, pressure_maximum_wind=22326.736328125, temperature_tropopause=209.85687255859375, geopotential_height_pblri=328.42779541015625, pressure_tropopause=21382.23828125, snow_depth_surface=0.052799999713897705, v-component_of_wind_tropopause=-33.18354797363281, downward_short_wave_rad_flux_surface=6.875, u-component_of_wind_maximum_wind=15.91766357421875, wilting_point_surface=0.04749999940395355, precipitable_water_entire_atmosphere=5.457814693450928, u-component_of_wind_pblri=2.274078369140625, direct_evaporation_cease_soil_moisture_surface=0.04749999940395355),\n",
       " Row(Timestamp=1426377600000, Geohash='f2fgf6usbe2p', geopotential_height_lltw=-472.96875, water_equiv_of_accum_snow_depth_surface=142.0, drag_coefficient_surface=0.0, sensible_heat_net_flux_surface=-7.07763671875, categorical_ice_pellets_yes1_no0_surface=0.0, visibility_surface=7621.5888671875, number_of_soil_layers_in_root_zone_surface=4.0, categorical_freezing_rain_yes1_no0_surface=0.0, pressure_reduced_to_msl_msl=101610.0, upward_short_wave_rad_flux_surface=0.0, relative_humidity_zerodegc_isotherm=77.0, categorical_snow_yes1_no0_surface=0.0, u-component_of_wind_tropopause=20.277877807617188, surface_wind_gust_surface=7.28378963470459, total_cloud_cover_entire_atmosphere=100.0, upward_long_wave_rad_flux_surface=291.5560302734375, land_cover_land1_sea0_surface=1.0, vegitation_type_as_in_sib_surface=1.0, v-component_of_wind_pblri=-1.68084716796875, albedo_surface=39.5, lightning_surface=0.0, ice_cover_ice1_no_ice0_surface=0.0, convective_inhibition_surface=-0.65234375, pressure_surface=97035.0, transpiration_stress-onset_soil_moisture_surface=0.3125, soil_porosity_surface=0.5, vegetation_surface=3.0, categorical_rain_yes1_no0_surface=0.0, downward_long_wave_rad_flux_surface=280.2162780761719, planetary_boundary_layer_height_surface=798.5, soil_type_as_in_zobler_surface=3.0, geopotential_height_cloud_base=1541.0, friction_velocity_surface=0.6258659958839417, maximumcomposite_radar_reflectivity_entire_atmosphere=11.25, plant_canopy_surface_water_surface=0.25999999046325684, v-component_of_wind_maximum_wind=-1.513824462890625, geopotential_height_zerodegc_isotherm=0.0, mean_sea_level_pressure_nam_model_reduction_msl=101625.0, temperature_surface=267.7480163574219, snow_cover_surface=100.0, geopotential_height_surface=362.32000732421875, convective_available_potential_energy_surface=0.0, latent_heat_net_flux_surface=12.717086791992188, surface_roughness_surface=1.9000158309936523, pressure_maximum_wind=12126.736328125, temperature_tropopause=219.73187255859375, geopotential_height_pblri=193.92779541015625, pressure_tropopause=25782.23828125, snow_depth_surface=0.5703999996185303, v-component_of_wind_tropopause=10.816452026367188, downward_short_wave_rad_flux_surface=0.0, u-component_of_wind_maximum_wind=30.91766357421875, wilting_point_surface=0.04749999940395355, precipitable_water_entire_atmosphere=9.082815170288086, u-component_of_wind_pblri=-5.475921630859375, direct_evaporation_cease_soil_moisture_surface=0.04749999940395355),\n",
       " Row(Timestamp=1426377600000, Geohash='9x786rfxxpzz', geopotential_height_lltw=3072.03125, water_equiv_of_accum_snow_depth_surface=0.0, drag_coefficient_surface=0.0, sensible_heat_net_flux_surface=-18.82763671875, categorical_ice_pellets_yes1_no0_surface=0.0, visibility_surface=24221.587890625, number_of_soil_layers_in_root_zone_surface=3.0, categorical_freezing_rain_yes1_no0_surface=0.0, pressure_reduced_to_msl_msl=102109.0, upward_short_wave_rad_flux_surface=29.25, relative_humidity_zerodegc_isotherm=49.0, categorical_snow_yes1_no0_surface=0.0, u-component_of_wind_tropopause=9.652877807617188, surface_wind_gust_surface=5.90878963470459, total_cloud_cover_entire_atmosphere=70.0, upward_long_wave_rad_flux_surface=344.6810302734375, land_cover_land1_sea0_surface=1.0, vegitation_type_as_in_sib_surface=10.0, v-component_of_wind_pblri=5.06915283203125, albedo_surface=19.25, lightning_surface=0.0, ice_cover_ice1_no_ice0_surface=0.0, convective_inhibition_surface=-0.65234375, pressure_surface=78016.0, transpiration_stress-onset_soil_moisture_surface=0.32999998331069946, soil_porosity_surface=0.5, vegetation_surface=3.0, categorical_rain_yes1_no0_surface=0.0, downward_long_wave_rad_flux_surface=262.8412780761719, planetary_boundary_layer_height_surface=2201.0, soil_type_as_in_zobler_surface=6.0, geopotential_height_cloud_base=8044.0, friction_velocity_surface=0.3258659839630127, maximumcomposite_radar_reflectivity_entire_atmosphere=-8.3125, plant_canopy_surface_water_surface=0.47999998927116394, v-component_of_wind_maximum_wind=-44.763824462890625, geopotential_height_zerodegc_isotherm=3440.0, mean_sea_level_pressure_nam_model_reduction_msl=101683.0, temperature_surface=279.1230163574219, snow_cover_surface=100.0, geopotential_height_surface=2258.570068359375, convective_available_potential_energy_surface=0.0, latent_heat_net_flux_surface=25.217086791992188, surface_roughness_surface=0.1750158965587616, pressure_maximum_wind=20326.736328125, temperature_tropopause=209.85687255859375, geopotential_height_pblri=193.67779541015625, pressure_tropopause=17782.23828125, snow_depth_surface=0.0007999999797903001, v-component_of_wind_tropopause=-40.30854797363281, downward_short_wave_rad_flux_surface=152.375, u-component_of_wind_maximum_wind=5.04266357421875, wilting_point_surface=0.06624999642372131, precipitable_water_entire_atmosphere=6.082814693450928, u-component_of_wind_pblri=5.836578369140625, direct_evaporation_cease_soil_moisture_surface=0.06624999642372131),\n",
       " Row(Timestamp=1426377600000, Geohash='cf36gb2z345b', geopotential_height_lltw=315.28125, water_equiv_of_accum_snow_depth_surface=82.0, drag_coefficient_surface=0.0, sensible_heat_net_flux_surface=-39.32763671875, categorical_ice_pellets_yes1_no0_surface=0.0, visibility_surface=24221.587890625, number_of_soil_layers_in_root_zone_surface=3.0, categorical_freezing_rain_yes1_no0_surface=0.0, pressure_reduced_to_msl_msl=99229.0, upward_short_wave_rad_flux_surface=7.875, relative_humidity_zerodegc_isotherm=23.0, categorical_snow_yes1_no0_surface=0.0, u-component_of_wind_tropopause=24.402877807617188, surface_wind_gust_surface=14.65878963470459, total_cloud_cover_entire_atmosphere=100.0, upward_long_wave_rad_flux_surface=315.6810302734375, land_cover_land1_sea0_surface=1.0, vegitation_type_as_in_sib_surface=18.0, v-component_of_wind_pblri=11.38165283203125, albedo_surface=35.0, lightning_surface=0.0, ice_cover_ice1_no_ice0_surface=0.0, convective_inhibition_surface=-0.65234375, pressure_surface=95390.0, transpiration_stress-onset_soil_moisture_surface=0.3125, soil_porosity_surface=0.5, vegetation_surface=1.0, categorical_rain_yes1_no0_surface=0.0, downward_long_wave_rad_flux_surface=310.3412780761719, planetary_boundary_layer_height_surface=1036.0, soil_type_as_in_zobler_surface=3.0, geopotential_height_cloud_base=1977.0, friction_velocity_surface=0.6758660078048706, maximumcomposite_radar_reflectivity_entire_atmosphere=8.8125, plant_canopy_surface_water_surface=0.13499999046325684, v-component_of_wind_maximum_wind=-11.763824462890625, geopotential_height_zerodegc_isotherm=2680.0, mean_sea_level_pressure_nam_model_reduction_msl=99250.0, temperature_surface=273.1230163574219, snow_cover_surface=100.0, geopotential_height_surface=321.32000732421875, convective_available_potential_energy_surface=0.0, latent_heat_net_flux_surface=9.092086791992188, surface_roughness_surface=0.1750158965587616, pressure_maximum_wind=11126.736328125, temperature_tropopause=208.10687255859375, geopotential_height_pblri=258.67779541015625, pressure_tropopause=20782.23828125, snow_depth_surface=0.3779999911785126, v-component_of_wind_tropopause=6.4414520263671875, downward_short_wave_rad_flux_surface=22.75, u-component_of_wind_maximum_wind=27.04266357421875, wilting_point_surface=0.04749999940395355, precipitable_water_entire_atmosphere=10.332815170288086, u-component_of_wind_pblri=0.274078369140625, direct_evaporation_cease_soil_moisture_surface=0.04749999940395355)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.snow_cover_surface > .85).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an SQL 'table'\n",
    "df.createOrReplaceTempView(\"TEMP_DF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unknown Feature\n",
    "I didn't know what albedo was, so I looked at its summary statistics. Still unsure, I looked up the definition: 'the proportion of the incident light or radiation that is reflected by a surface, typically that of a planet or moon.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|summary|  albedo_surface|\n",
      "+-------+----------------+\n",
      "|  count|             100|\n",
      "|   mean|           18.07|\n",
      "| stddev|17.4802948221907|\n",
      "|    min|             6.0|\n",
      "|    max|            76.0|\n",
      "+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('albedo_surface').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hot Hot Hot\n",
    "The hottest tempurature in the dataset is at {location} at {time}. Looking at the other highest tempuratures, it {does/not} appear to be an anomaly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempminmax = spark.sql(\"SELECT MIN(temperature_surface) as mintemp, MAX(temperature_surface) as maxtemp FROM TEMP_DF\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306.4980163574219"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(min(df.temperature_surface), max(df.temperature_surface)).first()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(temperature_surface=306.4980163574219),\n",
       " Row(temperature_surface=301.3730163574219),\n",
       " Row(temperature_surface=299.9980163574219),\n",
       " Row(temperature_surface=299.9980163574219),\n",
       " Row(temperature_surface=299.7480163574219),\n",
       " Row(temperature_surface=299.6230163574219),\n",
       " Row(temperature_surface=299.4980163574219),\n",
       " Row(temperature_surface=299.1230163574219),\n",
       " Row(temperature_surface=298.9980163574219),\n",
       " Row(temperature_surface=298.8730163574219)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hottest_record = df.orderBy(\"temperature_surface\", ascending=False).first()\n",
    "df.orderBy(\"temperature_surface\", ascending=False).select(\"temperature_surface\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|temperature_surface|\n",
      "+-------+-------------------+\n",
      "|  count|                100|\n",
      "|   mean|  284.9017663574219|\n",
      "| stddev| 13.002025568205239|\n",
      "|    min|          247.49802|\n",
      "|    max|          306.49802|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('temperature_surface').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2015, 3, 15, 0, 0, tzinfo=datetime.timezone.utc)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "timestamp = hottest_record['Timestamp'] / 1000\n",
    "datetime.fromtimestamp(timestamp, timezone.utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9qd23ynghrrz'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hottest_record['Geohash']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: double, _2: bigint]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = df.select(\"temperature_surface\").rdd\n",
    "\n",
    "#rdd.map(lambda row : row[0]).collect()\n",
    "#rdd.map(lambda row : row.temperature_surface).collect()\n",
    "hist_rdd = df.rdd.map(lambda row : row.temperature_surface).histogram(5)\n",
    "(start_vals, counts) = hist_rdd\n",
    "\n",
    "zipped = zip(start_vals, counts)\n",
    "type(zipped)\n",
    "hist_df = spark.createDataFrame(zipped)\n",
    "hist_df.createOrReplaceTempView(\"histogramTable\")\n",
    "\n",
    "# rdd = df.rdd.map(lambda row : row.temperature_surface)\n",
    "# help(spark.createDataFrame)\n",
    "# spark.createDataFrame(rdd, FloatType())\n",
    "# spark.createDataFrame(hist_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(snow_cover_surface=0.0)\n",
      "Row(snow_cover_surface=0.0)\n",
      "Row(snow_cover_surface=0.0)\n",
      "Row(snow_cover_surface=100.0)\n",
      "Row(snow_cover_surface=0.0)\n",
      "Row(snow_cover_surface=0.0)\n",
      "Row(snow_cover_surface=100.0)\n",
      "Row(snow_cover_surface=0.0)\n",
      "Row(snow_cover_surface=0.0)\n",
      "Row(snow_cover_surface=0.0)\n",
      "Max val observed: [Row(maxval=100.0)]\n"
     ]
    }
   ],
   "source": [
    "# Let's get all the snow cover values:\n",
    "snow = spark.sql(\"SELECT snow_cover_surface FROM TEMP_DF\").collect()\n",
    "# .collect() gives us a list of rows. Let's grab the first 10:\n",
    "for i in range(10):\n",
    "    print(snow[i])\n",
    "\n",
    "\n",
    "# What's the maximum value?\n",
    "snowmax = spark.sql(\"SELECT MAX(snow_cover_surface) as maxval FROM TEMP_DF\").collect()\n",
    "\n",
    "print('Max val observed:', snowmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on GroupedData in module pyspark.sql.group object:\n",
      "\n",
      "class GroupedData(builtins.object)\n",
      " |  A set of methods for aggregations on a :class:`DataFrame`,\n",
      " |  created by :func:`DataFrame.groupBy`.\n",
      " |  \n",
      " |  .. note:: Experimental\n",
      " |  \n",
      " |  .. versionadded:: 1.3\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, jgd, df)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  agg(self, *exprs)\n",
      " |      Compute aggregates and returns the result as a :class:`DataFrame`.\n",
      " |      \n",
      " |      The available aggregate functions are `avg`, `max`, `min`, `sum`, `count`.\n",
      " |      \n",
      " |      If ``exprs`` is a single :class:`dict` mapping from string to string, then the key\n",
      " |      is the column to perform aggregation on, and the value is the aggregate function.\n",
      " |      \n",
      " |      Alternatively, ``exprs`` can also be a list of aggregate :class:`Column` expressions.\n",
      " |      \n",
      " |      :param exprs: a dict mapping from column name (string) to aggregate functions (string),\n",
      " |          or a list of :class:`Column`.\n",
      " |      \n",
      " |      >>> gdf = df.groupBy(df.name)\n",
      " |      >>> sorted(gdf.agg({\"*\": \"count\"}).collect())\n",
      " |      [Row(name='Alice', count(1)=1), Row(name='Bob', count(1)=1)]\n",
      " |      \n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> sorted(gdf.agg(F.min(df.age)).collect())\n",
      " |      [Row(name='Alice', min(age)=2), Row(name='Bob', min(age)=5)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  apply(self, udf)\n",
      " |      Maps each group of the current :class:`DataFrame` using a pandas udf and returns the result\n",
      " |      as a `DataFrame`.\n",
      " |      \n",
      " |      The user-defined function should take a `pandas.DataFrame` and return another\n",
      " |      `pandas.DataFrame`. For each group, all columns are passed together as a `pandas.DataFrame`\n",
      " |      to the user-function and the returned `pandas.DataFrame`s are combined as a\n",
      " |      :class:`DataFrame`.\n",
      " |      The returned `pandas.DataFrame` can be of arbitrary length and its schema must match the\n",
      " |      returnType of the pandas udf.\n",
      " |      \n",
      " |      This function does not support partial aggregation, and requires shuffling all the data in\n",
      " |      the :class:`DataFrame`.\n",
      " |      \n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      :param udf: a grouped map user-defined function returned by\n",
      " |          :func:`pyspark.sql.functions.pandas_udf`.\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
      " |      ...     (\"id\", \"v\"))\n",
      " |      >>> @pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)  # doctest: +SKIP\n",
      " |      ... def normalize(pdf):\n",
      " |      ...     v = pdf.v\n",
      " |      ...     return pdf.assign(v=(v - v.mean()) / v.std())\n",
      " |      >>> df.groupby(\"id\").apply(normalize).show()  # doctest: +SKIP\n",
      " |      +---+-------------------+\n",
      " |      | id|                  v|\n",
      " |      +---+-------------------+\n",
      " |      |  1|-0.7071067811865475|\n",
      " |      |  1| 0.7071067811865475|\n",
      " |      |  2|-0.8320502943378437|\n",
      " |      |  2|-0.2773500981126146|\n",
      " |      |  2| 1.1094003924504583|\n",
      " |      +---+-------------------+\n",
      " |      \n",
      " |      .. seealso:: :meth:`pyspark.sql.functions.pandas_udf`\n",
      " |      \n",
      " |      .. versionadded:: 2.3\n",
      " |  \n",
      " |  avg(self, *cols)\n",
      " |      Computes average values for each numeric columns for each group.\n",
      " |      \n",
      " |      :func:`mean` is an alias for :func:`avg`.\n",
      " |      \n",
      " |      :param cols: list of column names (string). Non-numeric columns are ignored.\n",
      " |      \n",
      " |      >>> df.groupBy().avg('age').collect()\n",
      " |      [Row(avg(age)=3.5)]\n",
      " |      >>> df3.groupBy().avg('age', 'height').collect()\n",
      " |      [Row(avg(age)=3.5, avg(height)=82.5)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  count(self)\n",
      " |      Counts the number of records for each group.\n",
      " |      \n",
      " |      >>> sorted(df.groupBy(df.age).count().collect())\n",
      " |      [Row(age=2, count=1), Row(age=5, count=1)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  max(self, *cols)\n",
      " |      Computes the max value for each numeric columns for each group.\n",
      " |      \n",
      " |      >>> df.groupBy().max('age').collect()\n",
      " |      [Row(max(age)=5)]\n",
      " |      >>> df3.groupBy().max('age', 'height').collect()\n",
      " |      [Row(max(age)=5, max(height)=85)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  mean(self, *cols)\n",
      " |      Computes average values for each numeric columns for each group.\n",
      " |      \n",
      " |      :func:`mean` is an alias for :func:`avg`.\n",
      " |      \n",
      " |      :param cols: list of column names (string). Non-numeric columns are ignored.\n",
      " |      \n",
      " |      >>> df.groupBy().mean('age').collect()\n",
      " |      [Row(avg(age)=3.5)]\n",
      " |      >>> df3.groupBy().mean('age', 'height').collect()\n",
      " |      [Row(avg(age)=3.5, avg(height)=82.5)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  min(self, *cols)\n",
      " |      Computes the min value for each numeric column for each group.\n",
      " |      \n",
      " |      :param cols: list of column names (string). Non-numeric columns are ignored.\n",
      " |      \n",
      " |      >>> df.groupBy().min('age').collect()\n",
      " |      [Row(min(age)=2)]\n",
      " |      >>> df3.groupBy().min('age', 'height').collect()\n",
      " |      [Row(min(age)=2, min(height)=80)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  pivot(self, pivot_col, values=None)\n",
      " |      Pivots a column of the current :class:`DataFrame` and perform the specified aggregation.\n",
      " |      There are two versions of pivot function: one that requires the caller to specify the list\n",
      " |      of distinct values to pivot on, and one that does not. The latter is more concise but less\n",
      " |      efficient, because Spark needs to first compute the list of distinct values internally.\n",
      " |      \n",
      " |      :param pivot_col: Name of the column to pivot.\n",
      " |      :param values: List of values that will be translated to columns in the output DataFrame.\n",
      " |      \n",
      " |      # Compute the sum of earnings for each year by course with each course as a separate column\n",
      " |      \n",
      " |      >>> df4.groupBy(\"year\").pivot(\"course\", [\"dotNET\", \"Java\"]).sum(\"earnings\").collect()\n",
      " |      [Row(year=2012, dotNET=15000, Java=20000), Row(year=2013, dotNET=48000, Java=30000)]\n",
      " |      \n",
      " |      # Or without specifying column values (less efficient)\n",
      " |      \n",
      " |      >>> df4.groupBy(\"year\").pivot(\"course\").sum(\"earnings\").collect()\n",
      " |      [Row(year=2012, Java=20000, dotNET=15000), Row(year=2013, Java=30000, dotNET=48000)]\n",
      " |      \n",
      " |      .. versionadded:: 1.6\n",
      " |  \n",
      " |  sum(self, *cols)\n",
      " |      Compute the sum for each numeric columns for each group.\n",
      " |      \n",
      " |      :param cols: list of column names (string). Non-numeric columns are ignored.\n",
      " |      \n",
      " |      >>> df.groupBy().sum('age').collect()\n",
      " |      [Row(sum(age)=7)]\n",
      " |      >>> df3.groupBy().sum('age', 'height').collect()\n",
      " |      [Row(sum(age)=7, sum(height)=165)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So Snowy\n",
    "One location where it is snowing all year, i.e. where it is snowing for every record in its timeseries is {location}.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(geohash='c1nuq5290jup', sum=1.0, cnt=1, div=1.0),\n",
       " Row(geohash='f2w29r4werxb', sum=1.0, cnt=1, div=1.0),\n",
       " Row(geohash='f2d5v1jeyp7z', sum=1.0, cnt=1, div=1.0),\n",
       " Row(geohash='c6s64488ws80', sum=1.0, cnt=1, div=1.0),\n",
       " Row(geohash='f2fh6jpdgv5b', sum=1.0, cnt=1, div=1.0),\n",
       " Row(geohash='fccz22w4fytb', sum=1.0, cnt=1, div=1.0),\n",
       " Row(geohash='drmg1tprm22p', sum=0.0, cnt=1, div=0.0),\n",
       " Row(geohash='8gzfnvh85g0p', sum=0.0, cnt=1, div=0.0),\n",
       " Row(geohash='8up5c0e570pz', sum=0.0, cnt=1, div=0.0),\n",
       " Row(geohash='dkgtgw6v6y7z', sum=0.0, cnt=1, div=0.0)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gdf = df.groupBy('Geohash')\n",
    "# gdf.agg(sum('categorical_snow_yes1_no0_surface'), count('*')).orderBy('sum(categorical_snow_yes1_no0_surface)', ascending = False).collect()\n",
    "\n",
    "spark.sql(\"SELECT geohash, sum, cnt, (sum / cnt) AS div \\\n",
    "            FROM ( \\\n",
    "                SELECT geohash, SUM(categorical_snow_yes1_no0_surface) AS sum, COUNT(*) as cnt \\\n",
    "                FROM TEMP_DF \\\n",
    "                GROUP BY(geohash) \\\n",
    "            ) ORDER BY div DESC\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method agg in module pyspark.sql.group:\n",
      "\n",
      "agg(*exprs) method of pyspark.sql.group.GroupedData instance\n",
      "    Compute aggregates and returns the result as a :class:`DataFrame`.\n",
      "    \n",
      "    The available aggregate functions are `avg`, `max`, `min`, `sum`, `count`.\n",
      "    \n",
      "    If ``exprs`` is a single :class:`dict` mapping from string to string, then the key\n",
      "    is the column to perform aggregation on, and the value is the aggregate function.\n",
      "    \n",
      "    Alternatively, ``exprs`` can also be a list of aggregate :class:`Column` expressions.\n",
      "    \n",
      "    :param exprs: a dict mapping from column name (string) to aggregate functions (string),\n",
      "        or a list of :class:`Column`.\n",
      "    \n",
      "    >>> gdf = df.groupBy(df.name)\n",
      "    >>> sorted(gdf.agg({\"*\": \"count\"}).collect())\n",
      "    [Row(name='Alice', count(1)=1), Row(name='Bob', count(1)=1)]\n",
      "    \n",
      "    >>> from pyspark.sql import functions as F\n",
      "    >>> sorted(gdf.agg(F.min(df.age)).collect())\n",
      "    [Row(name='Alice', min(age)=2), Row(name='Bob', min(age)=5)]\n",
      "    \n",
      "    .. versionadded:: 1.3\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|avg(wilting_point_surface)|\n",
      "+--------------------------+\n",
      "|      0.029712499007582664|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df.select(avg(df.wilting_point_surface)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even create a sample dataset with Spark! Let's create a 10% sample (without replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = df.sample(False, .1)\n",
    "\n",
    "# Write it out to a file\n",
    "samp.write.format('csv').save('hdfs://orion12:50000/sampled_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
