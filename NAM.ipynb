{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up our Schema\n",
    "\n",
    "Spark can automatically create a schema for CSV files, but ours don't have headings. Let's set this up here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(Timestamp,LongType,true),StructField(Geohash,StringType,true),StructField(geopotential_height_lltw,FloatType,true),StructField(water_equiv_of_accum_snow_depth_surface,FloatType,true),StructField(drag_coefficient_surface,FloatType,true),StructField(sensible_heat_net_flux_surface,FloatType,true),StructField(categorical_ice_pellets_yes1_no0_surface,FloatType,true),StructField(visibility_surface,FloatType,true),StructField(number_of_soil_layers_in_root_zone_surface,FloatType,true),StructField(categorical_freezing_rain_yes1_no0_surface,FloatType,true),StructField(pressure_reduced_to_msl_msl,FloatType,true),StructField(upward_short_wave_rad_flux_surface,FloatType,true),StructField(relative_humidity_zerodegc_isotherm,FloatType,true),StructField(categorical_snow_yes1_no0_surface,FloatType,true),StructField(u-component_of_wind_tropopause,FloatType,true),StructField(surface_wind_gust_surface,FloatType,true),StructField(total_cloud_cover_entire_atmosphere,FloatType,true),StructField(upward_long_wave_rad_flux_surface,FloatType,true),StructField(land_cover_land1_sea0_surface,FloatType,true),StructField(vegitation_type_as_in_sib_surface,FloatType,true),StructField(v-component_of_wind_pblri,FloatType,true),StructField(albedo_surface,FloatType,true),StructField(lightning_surface,FloatType,true),StructField(ice_cover_ice1_no_ice0_surface,FloatType,true),StructField(convective_inhibition_surface,FloatType,true),StructField(pressure_surface,FloatType,true),StructField(transpiration_stress-onset_soil_moisture_surface,FloatType,true),StructField(soil_porosity_surface,FloatType,true),StructField(vegetation_surface,FloatType,true),StructField(categorical_rain_yes1_no0_surface,FloatType,true),StructField(downward_long_wave_rad_flux_surface,FloatType,true),StructField(planetary_boundary_layer_height_surface,FloatType,true),StructField(soil_type_as_in_zobler_surface,FloatType,true),StructField(geopotential_height_cloud_base,FloatType,true),StructField(friction_velocity_surface,FloatType,true),StructField(maximumcomposite_radar_reflectivity_entire_atmosphere,FloatType,true),StructField(plant_canopy_surface_water_surface,FloatType,true),StructField(v-component_of_wind_maximum_wind,FloatType,true),StructField(geopotential_height_zerodegc_isotherm,FloatType,true),StructField(mean_sea_level_pressure_nam_model_reduction_msl,FloatType,true),StructField(temperature_surface,FloatType,true),StructField(snow_cover_surface,FloatType,true),StructField(geopotential_height_surface,FloatType,true),StructField(convective_available_potential_energy_surface,FloatType,true),StructField(latent_heat_net_flux_surface,FloatType,true),StructField(surface_roughness_surface,FloatType,true),StructField(pressure_maximum_wind,FloatType,true),StructField(temperature_tropopause,FloatType,true),StructField(geopotential_height_pblri,FloatType,true),StructField(pressure_tropopause,FloatType,true),StructField(snow_depth_surface,FloatType,true),StructField(v-component_of_wind_tropopause,FloatType,true),StructField(downward_short_wave_rad_flux_surface,FloatType,true),StructField(u-component_of_wind_maximum_wind,FloatType,true),StructField(wilting_point_surface,FloatType,true),StructField(precipitable_water_entire_atmosphere,FloatType,true),StructField(u-component_of_wind_pblri,FloatType,true),StructField(direct_evaporation_cease_soil_moisture_surface,FloatType,true)))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, FloatType, LongType, StringType\n",
    "\n",
    "feats = []\n",
    "f = open('features.txt')\n",
    "for line_num, line in enumerate(f):\n",
    "    if line_num == 0:\n",
    "        # Timestamp\n",
    "        feats.append(StructField(line.strip(), LongType(), True))\n",
    "    elif line_num == 1:\n",
    "        # Geohash\n",
    "        feats.append(StructField(line.strip(), StringType(), True))\n",
    "    else:\n",
    "        # Other features\n",
    "        feats.append(StructField(line.strip(), FloatType(), True))\n",
    "    \n",
    "schema = StructType(feats)\n",
    "\n",
    "print(schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Dataframe\n",
    "\n",
    "Let's load our CSV into a 'dataframe' - Spark's abstraction for working with tabular data (built on top of RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nam_t = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:15000/nam_tiny.tdv')\n",
    "nam_s = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:15000/nam_s/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Timestamp: bigint, Geohash: string, geopotential_height_lltw: float, water_equiv_of_accum_snow_depth_surface: float, drag_coefficient_surface: float, sensible_heat_net_flux_surface: float, categorical_ice_pellets_yes1_no0_surface: float, visibility_surface: float, number_of_soil_layers_in_root_zone_surface: float, categorical_freezing_rain_yes1_no0_surface: float, pressure_reduced_to_msl_msl: float, upward_short_wave_rad_flux_surface: float, relative_humidity_zerodegc_isotherm: float, categorical_snow_yes1_no0_surface: float, u-component_of_wind_tropopause: float, surface_wind_gust_surface: float, total_cloud_cover_entire_atmosphere: float, upward_long_wave_rad_flux_surface: float, land_cover_land1_sea0_surface: float, vegitation_type_as_in_sib_surface: float, v-component_of_wind_pblri: float, albedo_surface: float, lightning_surface: float, ice_cover_ice1_no_ice0_surface: float, convective_inhibition_surface: float, pressure_surface: float, transpiration_stress-onset_soil_moisture_surface: float, soil_porosity_surface: float, vegetation_surface: float, categorical_rain_yes1_no0_surface: float, downward_long_wave_rad_flux_surface: float, planetary_boundary_layer_height_surface: float, soil_type_as_in_zobler_surface: float, geopotential_height_cloud_base: float, friction_velocity_surface: float, maximumcomposite_radar_reflectivity_entire_atmosphere: float, plant_canopy_surface_water_surface: float, v-component_of_wind_maximum_wind: float, geopotential_height_zerodegc_isotherm: float, mean_sea_level_pressure_nam_model_reduction_msl: float, temperature_surface: float, snow_cover_surface: float, geopotential_height_surface: float, convective_available_potential_energy_surface: float, latent_heat_net_flux_surface: float, surface_roughness_surface: float, pressure_maximum_wind: float, temperature_tropopause: float, geopotential_height_pblri: float, pressure_tropopause: float, snow_depth_surface: float, v-component_of_wind_tropopause: float, downward_short_wave_rad_flux_surface: float, u-component_of_wind_maximum_wind: float, wilting_point_surface: float, precipitable_water_entire_atmosphere: float, u-component_of_wind_pblri: float, direct_evaporation_cease_soil_moisture_surface: float]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nam_t.cache()\n",
    "nam_s.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Timestamp: bigint, Geohash: string, geopotential_height_lltw: float, water_equiv_of_accum_snow_depth_surface: float, drag_coefficient_surface: float, sensible_heat_net_flux_surface: float, categorical_ice_pellets_yes1_no0_surface: float, visibility_surface: float, number_of_soil_layers_in_root_zone_surface: float, categorical_freezing_rain_yes1_no0_surface: float, pressure_reduced_to_msl_msl: float, upward_short_wave_rad_flux_surface: float, relative_humidity_zerodegc_isotherm: float, categorical_snow_yes1_no0_surface: float, u-component_of_wind_tropopause: float, surface_wind_gust_surface: float, total_cloud_cover_entire_atmosphere: float, upward_long_wave_rad_flux_surface: float, land_cover_land1_sea0_surface: float, vegitation_type_as_in_sib_surface: float, v-component_of_wind_pblri: float, albedo_surface: float, lightning_surface: float, ice_cover_ice1_no_ice0_surface: float, convective_inhibition_surface: float, pressure_surface: float, transpiration_stress-onset_soil_moisture_surface: float, soil_porosity_surface: float, vegetation_surface: float, categorical_rain_yes1_no0_surface: float, downward_long_wave_rad_flux_surface: float, planetary_boundary_layer_height_surface: float, soil_type_as_in_zobler_surface: float, geopotential_height_cloud_base: float, friction_velocity_surface: float, maximumcomposite_radar_reflectivity_entire_atmosphere: float, plant_canopy_surface_water_surface: float, v-component_of_wind_maximum_wind: float, geopotential_height_zerodegc_isotherm: float, mean_sea_level_pressure_nam_model_reduction_msl: float, temperature_surface: float, snow_cover_surface: float, geopotential_height_surface: float, convective_available_potential_energy_surface: float, latent_heat_net_flux_surface: float, surface_roughness_surface: float, pressure_maximum_wind: float, temperature_tropopause: float, geopotential_height_pblri: float, pressure_tropopause: float, snow_depth_surface: float, v-component_of_wind_tropopause: float, downward_short_wave_rad_flux_surface: float, u-component_of_wind_maximum_wind: float, wilting_point_surface: float, precipitable_water_entire_atmosphere: float, u-component_of_wind_pblri: float, direct_evaporation_cease_soil_moisture_surface: float]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nam = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:15000/nam/*')\n",
    "nam.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an SQL 'table'\n",
    "nam_t.createOrReplaceTempView(\"nam_t\")\n",
    "nam_s.createOrReplaceTempView(\"nam_s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.cacheTable(\"nam_t\")\n",
    "spark.catalog.cacheTable(\"nam_s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nam.createOrReplaceTempView(\"nam\")\n",
    "spark.catalog.cacheTable(\"nam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unknown Feature\n",
    "I didn't know what albedo was, so I looked at its summary statistics. Still unsure, I looked up the definition: 'the proportion of the incident light or radiation that is reflected by a surface, typically that of a planet or moon.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|summary|  albedo_surface|\n",
      "+-------+----------------+\n",
      "|  count|             100|\n",
      "|   mean|           18.07|\n",
      "| stddev|17.4802948221907|\n",
      "|    min|             6.0|\n",
      "|    max|            76.0|\n",
      "+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nam_t.describe('albedo_surface').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hot Hot Hot\n",
    "The hottest tempurature in the dataset is 330.67431640625 at location d5f0jqerq27bat (21.13070154, -86.9520505; Benito Juárez, Quintana Roo, Mexico) at time 2015-08-23T18:00Z. This record is not surprising, as it is near the equator in the summer. Looking at the other highest tempuratures which are near this highest one, it does not appear to be an anomaly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Timestamp: bigint, Geohash: string, geopotential_height_lltw: float, water_equiv_of_accum_snow_depth_surface: float, drag_coefficient_surface: float, sensible_heat_net_flux_surface: float, categorical_ice_pellets_yes1_no0_surface: float, visibility_surface: float, number_of_soil_layers_in_root_zone_surface: float, categorical_freezing_rain_yes1_no0_surface: float, pressure_reduced_to_msl_msl: float, upward_short_wave_rad_flux_surface: float, relative_humidity_zerodegc_isotherm: float, categorical_snow_yes1_no0_surface: float, u-component_of_wind_tropopause: float, surface_wind_gust_surface: float, total_cloud_cover_entire_atmosphere: float, upward_long_wave_rad_flux_surface: float, land_cover_land1_sea0_surface: float, vegitation_type_as_in_sib_surface: float, v-component_of_wind_pblri: float, albedo_surface: float, lightning_surface: float, ice_cover_ice1_no_ice0_surface: float, convective_inhibition_surface: float, pressure_surface: float, transpiration_stress-onset_soil_moisture_surface: float, soil_porosity_surface: float, vegetation_surface: float, categorical_rain_yes1_no0_surface: float, downward_long_wave_rad_flux_surface: float, planetary_boundary_layer_height_surface: float, soil_type_as_in_zobler_surface: float, geopotential_height_cloud_base: float, friction_velocity_surface: float, maximumcomposite_radar_reflectivity_entire_atmosphere: float, plant_canopy_surface_water_surface: float, v-component_of_wind_maximum_wind: float, geopotential_height_zerodegc_isotherm: float, mean_sea_level_pressure_nam_model_reduction_msl: float, temperature_surface: float, snow_cover_surface: float, geopotential_height_surface: float, convective_available_potential_energy_surface: float, latent_heat_net_flux_surface: float, surface_roughness_surface: float, pressure_maximum_wind: float, temperature_tropopause: float, geopotential_height_pblri: float, pressure_tropopause: float, snow_depth_surface: float, v-component_of_wind_tropopause: float, downward_short_wave_rad_flux_surface: float, u-component_of_wind_maximum_wind: float, wilting_point_surface: float, precipitable_water_entire_atmosphere: float, u-component_of_wind_pblri: float, direct_evaporation_cease_soil_moisture_surface: float]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_desc = nam_s.orderBy(\"temperature_surface\", ascending=False)\n",
    "temp_desc.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hottest_record = temp_desc.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330.67431640625"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hottest_record['temperature_surface']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2015, 8, 23, 18, 0, tzinfo=datetime.timezone.utc)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "timestamp = hottest_record['Timestamp'] / 1000\n",
    "datetime.fromtimestamp(timestamp, timezone.utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d5f0jqerq27b'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hottest_record['Geohash']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(temperature_surface=330.67431640625, Timestamp=1440352800000, Geohash='d5f0jqerq27b'),\n",
       " Row(temperature_surface=330.640625, Timestamp=1440266400000, Geohash='d5f0vd8eb80p'),\n",
       " Row(temperature_surface=330.6044921875, Timestamp=1430157600000, Geohash='9g77js659k20'),\n",
       " Row(temperature_surface=330.53662109375, Timestamp=1439056800000, Geohash='d5f0jqerq27b'),\n",
       " Row(temperature_surface=330.48193359375, Timestamp=1440612000000, Geohash='d59d5yttuc5b'),\n",
       " Row(temperature_surface=330.35693359375, Timestamp=1440612000000, Geohash='d59eqv7e03pb'),\n",
       " Row(temperature_surface=330.23193359375, Timestamp=1440612000000, Geohash='d59dntd726gz'),\n",
       " Row(temperature_surface=330.220703125, Timestamp=1440698400000, Geohash='d59eqv7e03pb'),\n",
       " Row(temperature_surface=330.179931640625, Timestamp=1438279200000, Geohash='d5f04xyhucez'),\n",
       " Row(temperature_surface=330.14990234375, Timestamp=1439488800000, Geohash='d5dpds10m55b')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_desc.select(\"temperature_surface\", \"Timestamp\", \"Geohash\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Timestamp: bigint, Geohash: string, geopotential_height_lltw: float, water_equiv_of_accum_snow_depth_surface: float, drag_coefficient_surface: float, sensible_heat_net_flux_surface: float, categorical_ice_pellets_yes1_no0_surface: float, visibility_surface: float, number_of_soil_layers_in_root_zone_surface: float, categorical_freezing_rain_yes1_no0_surface: float, pressure_reduced_to_msl_msl: float, upward_short_wave_rad_flux_surface: float, relative_humidity_zerodegc_isotherm: float, categorical_snow_yes1_no0_surface: float, u-component_of_wind_tropopause: float, surface_wind_gust_surface: float, total_cloud_cover_entire_atmosphere: float, upward_long_wave_rad_flux_surface: float, land_cover_land1_sea0_surface: float, vegitation_type_as_in_sib_surface: float, v-component_of_wind_pblri: float, albedo_surface: float, lightning_surface: float, ice_cover_ice1_no_ice0_surface: float, convective_inhibition_surface: float, pressure_surface: float, transpiration_stress-onset_soil_moisture_surface: float, soil_porosity_surface: float, vegetation_surface: float, categorical_rain_yes1_no0_surface: float, downward_long_wave_rad_flux_surface: float, planetary_boundary_layer_height_surface: float, soil_type_as_in_zobler_surface: float, geopotential_height_cloud_base: float, friction_velocity_surface: float, maximumcomposite_radar_reflectivity_entire_atmosphere: float, plant_canopy_surface_water_surface: float, v-component_of_wind_maximum_wind: float, geopotential_height_zerodegc_isotherm: float, mean_sea_level_pressure_nam_model_reduction_msl: float, temperature_surface: float, snow_cover_surface: float, geopotential_height_surface: float, convective_available_potential_energy_surface: float, latent_heat_net_flux_surface: float, surface_roughness_surface: float, pressure_maximum_wind: float, temperature_tropopause: float, geopotential_height_pblri: float, pressure_tropopause: float, snow_depth_surface: float, v-component_of_wind_tropopause: float, downward_short_wave_rad_flux_surface: float, u-component_of_wind_maximum_wind: float, wilting_point_surface: float, precipitable_water_entire_atmosphere: float, u-component_of_wind_pblri: float, direct_evaporation_cease_soil_moisture_surface: float]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_desc.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So Snowy\n",
    "The location where it snows most often is c41uhb4r5n00 (56.9543589, -132.32710345; City and Borough of Wrangell, AK). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[geohash: string, sum: double, cnt: bigint, div: double]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "so_snowy_s = spark.sql(\"SELECT geohash, sum, cnt, (sum / cnt) AS div \\\n",
    "            FROM ( \\\n",
    "                SELECT geohash, SUM(categorical_snow_yes1_no0_surface) AS sum, COUNT(*) as cnt \\\n",
    "                FROM nam_s \\\n",
    "                GROUP BY(geohash) \\\n",
    "            ) ORDER BY div DESC\")\n",
    "so_snowy_s.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(geohash='c41uhb4r5n00', sum=168.0, cnt=421, div=0.3990498812351544),\n",
       " Row(geohash='c45277s4gjpb', sum=163.0, cnt=423, div=0.38534278959810875),\n",
       " Row(geohash='c44jc11cn1rz', sum=165.0, cnt=433, div=0.3810623556581986),\n",
       " Row(geohash='c41yek3dwk2p', sum=152.0, cnt=403, div=0.3771712158808933),\n",
       " Row(geohash='c1uz20wg2gxb', sum=141.0, cnt=376, div=0.375),\n",
       " Row(geohash='c439n53vsxzz', sum=155.0, cnt=417, div=0.37170263788968827),\n",
       " Row(geohash='c1gv86v08280', sum=148.0, cnt=401, div=0.3690773067331671),\n",
       " Row(geohash='c1gy5p6c9n2p', sum=150.0, cnt=407, div=0.36855036855036855),\n",
       " Row(geohash='c41uxkww12rz', sum=148.0, cnt=403, div=0.36724565756823824),\n",
       " Row(geohash='c1gru77j5fzz', sum=167.0, cnt=455, div=0.367032967032967),\n",
       " Row(geohash='c44srx2d2tzz', sum=158.0, cnt=431, div=0.3665893271461717),\n",
       " Row(geohash='c43kcu3t702p', sum=149.0, cnt=409, div=0.3643031784841076),\n",
       " Row(geohash='c43k6uu1egxb', sum=150.0, cnt=412, div=0.3640776699029126),\n",
       " Row(geohash='c44j614gx8xb', sum=141.0, cnt=389, div=0.36246786632390743),\n",
       " Row(geohash='c438x5esgf00', sum=143.0, cnt=395, div=0.3620253164556962),\n",
       " Row(geohash='c43hr0kbbvpb', sum=158.0, cnt=437, div=0.36155606407322655),\n",
       " Row(geohash='c1vqm5v4umpb', sum=155.0, cnt=430, div=0.36046511627906974),\n",
       " Row(geohash='c432ydkuzy2p', sum=145.0, cnt=403, div=0.3598014888337469),\n",
       " Row(geohash='c41v48pupf00', sum=155.0, cnt=431, div=0.35962877030162416),\n",
       " Row(geohash='c438fqgmsm00', sum=157.0, cnt=438, div=0.3584474885844749),\n",
       " Row(geohash='c43h57tx92xb', sum=140.0, cnt=391, div=0.35805626598465473),\n",
       " Row(geohash='c41z4k0q1v00', sum=136.0, cnt=380, div=0.35789473684210527),\n",
       " Row(geohash='c41vnktkn7xb', sum=151.0, cnt=424, div=0.3561320754716981),\n",
       " Row(geohash='c43kp9tv6krz', sum=141.0, cnt=396, div=0.3560606060606061),\n",
       " Row(geohash='c41ueb1jyypb', sum=143.0, cnt=402, div=0.35572139303482586),\n",
       " Row(geohash='c1vrpbjs6180', sum=133.0, cnt=374, div=0.35561497326203206),\n",
       " Row(geohash='c452rpbu2mbp', sum=143.0, cnt=403, div=0.3548387096774194),\n",
       " Row(geohash='c41vtks3952p', sum=143.0, cnt=404, div=0.35396039603960394),\n",
       " Row(geohash='c450pxr2zbxb', sum=139.0, cnt=393, div=0.35368956743002544),\n",
       " Row(geohash='c3041ty0ftpb', sum=146.0, cnt=414, div=0.3526570048309179),\n",
       " Row(geohash='c43s5qjhy0xb', sum=139.0, cnt=395, div=0.3518987341772152),\n",
       " Row(geohash='c1gvsp10dmzz', sum=158.0, cnt=449, div=0.3518930957683742),\n",
       " Row(geohash='c432rdwup080', sum=139.0, cnt=397, div=0.3501259445843829),\n",
       " Row(geohash='c41z9hnc43rz', sum=142.0, cnt=407, div=0.3488943488943489),\n",
       " Row(geohash='c4459s8rz1xb', sum=137.0, cnt=393, div=0.3486005089058524),\n",
       " Row(geohash='c43b05v7222p', sum=137.0, cnt=393, div=0.3486005089058524),\n",
       " Row(geohash='c4qm97202jup', sum=136.0, cnt=392, div=0.3469387755102041),\n",
       " Row(geohash='c41yhk7n3n00', sum=133.0, cnt=386, div=0.344559585492228),\n",
       " Row(geohash='c1gwp7n9ecbp', sum=152.0, cnt=442, div=0.3438914027149321),\n",
       " Row(geohash='c1vx5s4d8u7z', sum=131.0, cnt=381, div=0.3438320209973753),\n",
       " Row(geohash='c1tn7vh1phxb', sum=142.0, cnt=413, div=0.34382566585956414),\n",
       " Row(geohash='c1grzzxwt1bp', sum=143.0, cnt=416, div=0.34375),\n",
       " Row(geohash='c41x3g48q200', sum=135.0, cnt=393, div=0.3435114503816794),\n",
       " Row(geohash='c1vw2zccus80', sum=139.0, cnt=405, div=0.3432098765432099),\n",
       " Row(geohash='c44s7ffyr9rz', sum=141.0, cnt=411, div=0.34306569343065696),\n",
       " Row(geohash='c41wrxgfefpb', sum=136.0, cnt=397, div=0.3425692695214106),\n",
       " Row(geohash='c437gvnt3d00', sum=138.0, cnt=403, div=0.3424317617866005),\n",
       " Row(geohash='c41xmx88zn80', sum=136.0, cnt=398, div=0.3417085427135678),\n",
       " Row(geohash='c453n01c9exb', sum=142.0, cnt=416, div=0.34134615384615385),\n",
       " Row(geohash='c43sdmg10700', sum=130.0, cnt=381, div=0.34120734908136485),\n",
       " Row(geohash='c41xurr50ypb', sum=139.0, cnt=409, div=0.33985330073349634),\n",
       " Row(geohash='c44uh5wjf92p', sum=140.0, cnt=412, div=0.33980582524271846),\n",
       " Row(geohash='c4387rjkb0rz', sum=149.0, cnt=439, div=0.33940774487471526),\n",
       " Row(geohash='c37rkxnm3cup', sum=133.0, cnt=392, div=0.3392857142857143),\n",
       " Row(geohash='c1p7nnvm1g5b', sum=144.0, cnt=425, div=0.3388235294117647),\n",
       " Row(geohash='c1sgmt7hzg80', sum=132.0, cnt=390, div=0.3384615384615385),\n",
       " Row(geohash='c1uyc0k42crz', sum=136.0, cnt=405, div=0.3358024691358025),\n",
       " Row(geohash='c44hk1hy9u2p', sum=138.0, cnt=411, div=0.3357664233576642),\n",
       " Row(geohash='c1gr1wvsqzrz', sum=139.0, cnt=414, div=0.3357487922705314),\n",
       " Row(geohash='c1grm74n9f00', sum=139.0, cnt=414, div=0.3357487922705314),\n",
       " Row(geohash='c37p9qfx2x80', sum=140.0, cnt=417, div=0.33573141486810554),\n",
       " Row(geohash='c4393q9myw00', sum=142.0, cnt=424, div=0.33490566037735847),\n",
       " Row(geohash='c44eufgnnq2p', sum=132.0, cnt=395, div=0.3341772151898734),\n",
       " Row(geohash='c44hg15vqhxb', sum=135.0, cnt=404, div=0.3341584158415842),\n",
       " Row(geohash='c433md4vjx80', sum=136.0, cnt=408, div=0.3333333333333333),\n",
       " Row(geohash='fd9fuvhc2sxb', sum=133.0, cnt=399, div=0.3333333333333333),\n",
       " Row(geohash='c4n2kr7y2rs0', sum=133.0, cnt=399, div=0.3333333333333333),\n",
       " Row(geohash='c1t540c4vjh0', sum=138.0, cnt=415, div=0.3325301204819277),\n",
       " Row(geohash='c44syx23gepb', sum=135.0, cnt=406, div=0.33251231527093594),\n",
       " Row(geohash='c1t384wx6srz', sum=130.0, cnt=391, div=0.33248081841432225),\n",
       " Row(geohash='fd9gmu8sjuup', sum=144.0, cnt=434, div=0.3317972350230415),\n",
       " Row(geohash='c1vmy53xcjzz', sum=139.0, cnt=419, div=0.3317422434367542),\n",
       " Row(geohash='fd9g109d19gz', sum=134.0, cnt=404, div=0.3316831683168317),\n",
       " Row(geohash='c1vqb85vdvh0', sum=136.0, cnt=412, div=0.3300970873786408),\n",
       " Row(geohash='c4n22ewhww00', sum=135.0, cnt=409, div=0.33007334963325186),\n",
       " Row(geohash='c1nrwpc5kpkp', sum=150.0, cnt=455, div=0.32967032967032966),\n",
       " Row(geohash='c1th060z91up', sum=142.0, cnt=431, div=0.3294663573085847),\n",
       " Row(geohash='c445v1jx5z2p', sum=138.0, cnt=419, div=0.32935560859188545),\n",
       " Row(geohash='c3k396yhyuup', sum=132.0, cnt=401, div=0.32917705735660846),\n",
       " Row(geohash='c4j7z4pc5rxb', sum=132.0, cnt=401, div=0.32917705735660846),\n",
       " Row(geohash='c1t3hw59skzz', sum=136.0, cnt=416, div=0.3269230769230769),\n",
       " Row(geohash='c43mj8cjkdpb', sum=127.0, cnt=389, div=0.3264781491002571),\n",
       " Row(geohash='c44v95tupg80', sum=135.0, cnt=414, div=0.32608695652173914),\n",
       " Row(geohash='c1vws7y8prbp', sum=131.0, cnt=402, div=0.32587064676616917),\n",
       " Row(geohash='c1gx2zw2p3bp', sum=139.0, cnt=428, div=0.3247663551401869),\n",
       " Row(geohash='c4qfp6fm69bp', sum=134.0, cnt=413, div=0.324455205811138),\n",
       " Row(geohash='c3k2d8k3h7pb', sum=133.0, cnt=410, div=0.32439024390243903),\n",
       " Row(geohash='c3k91p7mjeh0', sum=130.0, cnt=401, div=0.32418952618453867),\n",
       " Row(geohash='c1gr8xnduyrz', sum=128.0, cnt=395, div=0.3240506329113924),\n",
       " Row(geohash='c44emfgm6r2p', sum=136.0, cnt=420, div=0.3238095238095238),\n",
       " Row(geohash='c3k89wn4zes0', sum=135.0, cnt=417, div=0.3237410071942446),\n",
       " Row(geohash='c43m2snbpzbp', sum=133.0, cnt=411, div=0.3236009732360097),\n",
       " Row(geohash='c4qsrnzct8kp', sum=131.0, cnt=405, div=0.3234567901234568),\n",
       " Row(geohash='c36m6mkpn3rz', sum=130.0, cnt=402, div=0.32338308457711445),\n",
       " Row(geohash='c4jk4whz827z', sum=129.0, cnt=399, div=0.3233082706766917),\n",
       " Row(geohash='c43j14zyhszz', sum=134.0, cnt=415, div=0.3228915662650602),\n",
       " Row(geohash='c4qupwjv0mgz', sum=138.0, cnt=428, div=0.32242990654205606),\n",
       " Row(geohash='c37r3h5gq7eb', sum=137.0, cnt=425, div=0.32235294117647056),\n",
       " Row(geohash='c3e028j8ggkp', sum=137.0, cnt=425, div=0.32235294117647056),\n",
       " Row(geohash='c3k508c9y1s0', sum=126.0, cnt=391, div=0.32225063938618925)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "so_snowy_s.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_snowy = spark.sql(\"SELECT geohash, sum, cnt, (sum / cnt) AS div \\\n",
    "            FROM ( \\\n",
    "                SELECT geohash, SUM(categorical_snow_yes1_no0_surface) AS sum, COUNT(*) as cnt \\\n",
    "                FROM nam \\\n",
    "                GROUP BY(geohash) \\\n",
    "            ) \\\n",
    "            ORDER BY div DESC \\\n",
    "            LIMIT 100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o283.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 37.0 failed 4 times, most recent failure: Lost task 1.3 in stage 37.0 (TID 937, 10.0.1.21, executor 2): java.io.IOException: No space left on device\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:65)\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:314)\n\tat java.nio.channels.Channels.writeFullyImpl(Channels.java:78)\n\tat java.nio.channels.Channels.writeFully(Channels.java:101)\n\tat java.nio.channels.Channels.access$000(Channels.java:61)\n\tat java.nio.channels.Channels$1.write(Channels.java:174)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:220)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:173)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.writeByte(ObjectOutputStream.java:1915)\n\tat java.io.ObjectOutputStream.writeFatalException(ObjectOutputStream.java:1576)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:351)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:174)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1101)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1099)\n\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:68)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1099)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2131)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1029)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1433)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1420)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:135)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3195)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: No space left on device\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:65)\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:314)\n\tat java.nio.channels.Channels.writeFullyImpl(Channels.java:78)\n\tat java.nio.channels.Channels.writeFully(Channels.java:101)\n\tat java.nio.channels.Channels.access$000(Channels.java:61)\n\tat java.nio.channels.Channels$1.write(Channels.java:174)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:220)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:173)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.writeByte(ObjectOutputStream.java:1915)\n\tat java.io.ObjectOutputStream.writeFatalException(ObjectOutputStream.java:1576)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:351)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:174)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1101)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1099)\n\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:68)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1099)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-6a08393cee46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mso_snowy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1132\u001b[0m             \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \"\"\"\n\u001b[0;32m--> 504\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \"\"\"\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.3.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.3.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o283.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 37.0 failed 4 times, most recent failure: Lost task 1.3 in stage 37.0 (TID 937, 10.0.1.21, executor 2): java.io.IOException: No space left on device\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:65)\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:314)\n\tat java.nio.channels.Channels.writeFullyImpl(Channels.java:78)\n\tat java.nio.channels.Channels.writeFully(Channels.java:101)\n\tat java.nio.channels.Channels.access$000(Channels.java:61)\n\tat java.nio.channels.Channels$1.write(Channels.java:174)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:220)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:173)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.writeByte(ObjectOutputStream.java:1915)\n\tat java.io.ObjectOutputStream.writeFatalException(ObjectOutputStream.java:1576)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:351)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:174)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1101)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1099)\n\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:68)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1099)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2131)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1029)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1433)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1420)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:135)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3195)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: No space left on device\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:65)\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:314)\n\tat java.nio.channels.Channels.writeFullyImpl(Channels.java:78)\n\tat java.nio.channels.Channels.writeFully(Channels.java:101)\n\tat java.nio.channels.Channels.access$000(Channels.java:61)\n\tat java.nio.channels.Channels$1.write(Channels.java:174)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:220)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:173)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.writeByte(ObjectOutputStream.java:1915)\n\tat java.io.ObjectOutputStream.writeFatalException(ObjectOutputStream.java:1576)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:351)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:174)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1101)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1099)\n\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:68)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1099)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "so_snowy.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strangely Snowy \n",
    "Find a location that contains snow while its surroundings do not. Why does this occur? Is it a high mountain peak in a desert?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning rod\n",
    "Where are you most likely to be struck by lightning? Use a precision of at least 4 Geohash characters and provide the top 3 locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_rod = spark.sql(\"\\\n",
    "                           SELECT LEFT(geohash, 4) AS geo4, SUM(lightning_surface) / COUNT(*) AS prob_lightning \\\n",
    "                           FROM nam_s \\\n",
    "                           GROUP BY geo4 \\\n",
    "                           ORDER BY prob_lightning DESC \\\n",
    "                           LIMIT 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(geo4='9g3v', prob_lightning=0.3184),\n",
       " Row(geo4='9g3h', prob_lightning=0.29589905362776026),\n",
       " Row(geo4='9g3m', prob_lightning=0.29389942291838417)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightning_rod.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drying out\n",
    "(Choose a region in North America (defined by one or more Geohashes) and determine when its driest month is. This should include a histogram with data from each month.)\n",
    "\n",
    "In the San Francisco Bay Area, the driest (i.e. least humid) month is August, as seen in the humidity values averaged for each month below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nam_s_bayarea = spark.read.format('csv').option('sep', ',').schema(schema).load('hdfs://orion11:15000/nam_s_bayarea/*')\n",
    "nam_s_bayarea.createOrReplaceTempView(\"nam_s_bayarea\")\n",
    "spark.catalog.cacheTable(\"nam_s_bayarea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "drying_out = spark.sql(\"\\\n",
    "    SELECT \\\n",
    "        trunc(from_unixtime(timestamp DIV 1000), 'MM') AS year_month, \\\n",
    "        AVG(relative_humidity_zerodegc_isotherm) AS avg_humidity, \\\n",
    "        COUNT(*) AS count \\\n",
    "    FROM nam_s_bayarea \\\n",
    "    GROUP BY year_month \\\n",
    "    ORDER BY year_month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----+\n",
      "|year_month|      avg_humidity|count|\n",
      "+----------+------------------+-----+\n",
      "|2015-01-01|31.324879025529786| 5993|\n",
      "|2015-02-01|28.585786986550346| 5502|\n",
      "|2015-03-01| 23.24552213585671| 5918|\n",
      "|2015-04-01| 35.07409362900387| 5682|\n",
      "|2015-05-01| 30.44884776536313| 5728|\n",
      "|2015-06-01| 25.14540059347181| 5729|\n",
      "|2015-07-01|35.376506024096386| 5644|\n",
      "|2015-08-01|22.981870400287203| 5571|\n",
      "|2015-09-01| 29.28472104459895| 5897|\n",
      "|2015-10-01|31.750963222416814| 5710|\n",
      "|2015-11-01|38.616337929830685| 5729|\n",
      "|2015-12-01| 50.35549132947977| 5536|\n",
      "+----------+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drying_out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_humidity = drying_out.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "months = list(map(lambda row: row['year_month'].month, monthly_humidity))\n",
    "months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31.324879025529786,\n",
       " 28.585786986550346,\n",
       " 23.24552213585671,\n",
       " 35.07409362900387,\n",
       " 30.44884776536313,\n",
       " 25.14540059347181,\n",
       " 35.376506024096386,\n",
       " 22.981870400287203,\n",
       " 29.28472104459895,\n",
       " 31.750963222416814,\n",
       " 38.616337929830685,\n",
       " 50.35549132947977]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "humidities = list(map(lambda row: row['avg_humidity'], monthly_humidity))\n",
    "humidities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.981870400287203"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(humidities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar(labels, values, xlabel = \"\", ylabel = \"\", title = \"\"):\n",
    "    positions = range(len(labels))\n",
    "    plt.bar(positions, values)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xticks(positions, labels)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_bar(months, humidities, \n",
    "           xlabel = \"month\",\n",
    "           ylabel = \"humidity (avg)\",\n",
    "           title = \"Humidity: Bay Area, 2015\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travel Startup\n",
    "After graduating from USF, you found a startup that aims to provide personalized travel itineraries using big data analysis. Given your own personal preferences, build a plan for a year of travel across 5 locations. Or, in other words: pick 5 regions. What is the best time of year to visit them based on the dataset?\n",
    "\n",
    "- One avenue here could be determining the comfort index for a region. You could incorporate several features: not too hot, not too cold, dry, humid, windy, etc. There are several different ways of calculating this available online, and you could also analyze how well your own metrics do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escaping the fog\n",
    "After becoming rich from your startup, you are looking for the perfect location to build your Bay Area mansion with unobstructed views. Find the locations that are the least foggy and show them on a map.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "escaping_fog = spark.sql(\"\\\n",
    "SELECT \\\n",
    "    LEFT(geohash, 4) AS geo4, \\\n",
    "    AVG(visibility_surface) AS avg_visibility \\\n",
    "    COUNT(*) AS count \\\n",
    "FROM nam_s_bayarea \\\n",
    "GROUP BY geo4 \\\n",
    "ORDER BY avg_visibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|geo4|    avg_visibility|\n",
      "+----+------------------+\n",
      "|9qb9|18866.500605645666|\n",
      "|9qb6|20399.862331219643|\n",
      "|9qb8|20552.363280207388|\n",
      "|9q8u|20594.105991876255|\n",
      "|9qb3| 20937.62078815212|\n",
      "|9q8v| 20957.84047165712|\n",
      "|9qb7| 21085.58722571984|\n",
      "|9q8y|21230.055428700784|\n",
      "|9qbd|21285.232498519046|\n",
      "|9q8x|21311.084569252158|\n",
      "|9q8g| 21326.57320909403|\n",
      "|9qbh|21367.223755599265|\n",
      "|9qbc| 21545.05316134313|\n",
      "|9qbk|21968.482188283397|\n",
      "|9qbf| 22235.38120562749|\n",
      "|9q9h|22280.085210652833|\n",
      "|9q8z|22605.161260960067|\n",
      "|9q9n|22618.017425336275|\n",
      "|9q9k|22723.900763805847|\n",
      "|9q9m| 22729.76162779337|\n",
      "+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "escaping_fog.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SolarWind, Inc.\n",
    "You get bored enjoying the amazing views from your mansion, so you start a new company; here, you want to help power companies plan out the locations of solar and wind farms across North America. Locate the top 3 places for solar and wind farms, as well as a combination of both (solar + wind farm). You will report a total of 9 Geohashes as well as their relevant attributes (for example, cloud cover and wind speeds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even create a sample dataset with Spark! Let's create a 10% sample (without replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = df.sample(False, .1)\n",
    "\n",
    "# Write it out to a file\n",
    "samp.write.format('csv').save('hdfs://orion12:50000/sampled_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "nam_s_bayarea = spark.sql(\"\\\n",
    "SELECT * \\\n",
    "FROM nam_s \\\n",
    "WHERE LEFT(geohash, 4) IN ( \\\n",
    " '9q8x', '9q8z', '9q8y', '9q8v', '9q8u', '9q8g', \\\n",
    " '9q9p', '9q9n', '9q9j', '9q9h', '9q95', '9q9r', '9q9q', '9q9m', '9q9k', '9q97', '9q9e', \\\n",
    " '9qb3', '9q95', '9qb6', '9qb7', '9qb8', '9qb9', '9qbb', '9qbc', '9qbd', '9qbe', '9qbf', '9qbg', '9qbh', '9qbk', '9qbs', \\\n",
    " '9qc0', '9qc1', '9qc2', '9qc3', '9qc4', '9qc5', '9qc6', '9qch' \\\n",
    ")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "nam_s_bayarea.write.format('csv').save('hdfs://orion11:15000/nam_s_bayarea')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
